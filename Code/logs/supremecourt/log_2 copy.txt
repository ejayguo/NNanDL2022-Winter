[2022/4/17 19:25:32] Epoch: 1, train_loss= 2.57277, train_acc= 0.01328, val_loss= 2.56188, val_acc= 0.23425, time= 24.93298
[2022/4/17 19:25:51] Epoch: 2, train_loss= 2.53416, train_acc= 0.24681, val_loss= 2.56035, val_acc= 0.25040, time= 18.69860
[2022/4/17 19:26:05] Epoch: 3, train_loss= 2.51945, train_acc= 0.26476, val_loss= 2.55950, val_acc= 0.39580, time= 13.75880
[2022/4/17 19:26:15] Epoch: 4, train_loss= 2.51190, train_acc= 0.40262, val_loss= 2.55876, val_acc= 0.45073, time= 10.51725
[2022/4/17 19:26:26] Epoch: 5, train_loss= 2.50508, train_acc= 0.47586, val_loss= 2.55801, val_acc= 0.47658, time= 10.15356
[2022/4/17 19:26:36] Epoch: 6, train_loss= 2.49797, train_acc= 0.51678, val_loss= 2.55726, val_acc= 0.49111, time= 9.93496
[2022/4/17 19:26:56] Epoch: 7, train_loss= 2.49080, train_acc= 0.53653, val_loss= 2.55651, val_acc= 0.51373, time= 20.31229
[2022/4/17 19:27:10] Epoch: 8, train_loss= 2.48360, train_acc= 0.55484, val_loss= 2.55577, val_acc= 0.52181, time= 14.41300
[2022/4/17 19:27:23] Epoch: 9, train_loss= 2.47644, train_acc= 0.55897, val_loss= 2.55512, val_acc= 0.53150, time= 12.61338
[2022/4/17 19:27:41] Epoch: 10, train_loss= 2.46999, train_acc= 0.55950, val_loss= 2.55457, val_acc= 0.53150, time= 18.08468
[2022/4/17 19:28:07] Epoch: 11, train_loss= 2.46422, train_acc= 0.56884, val_loss= 2.55401, val_acc= 0.54604, time= 25.58883
[2022/4/17 19:28:33] Epoch: 12, train_loss= 2.45853, train_acc= 0.58122, val_loss= 2.55343, val_acc= 0.56381, time= 26.32286
[2022/4/17 19:28:58] Epoch: 13, train_loss= 2.45284, train_acc= 0.60707, val_loss= 2.55287, val_acc= 0.58481, time= 25.45115
[2022/4/17 19:29:26] Epoch: 14, train_loss= 2.44749, train_acc= 0.63454, val_loss= 2.55236, val_acc= 0.60582, time= 27.24195
[2022/4/17 19:29:53] Epoch: 15, train_loss= 2.44248, train_acc= 0.65931, val_loss= 2.55188, val_acc= 0.62843, time= 27.10250
[2022/4/17 19:30:21] Epoch: 16, train_loss= 2.43758, train_acc= 0.67654, val_loss= 2.55145, val_acc= 0.63328, time= 28.53302
[2022/4/17 19:30:46] Epoch: 17, train_loss= 2.43292, train_acc= 0.68192, val_loss= 2.55107, val_acc= 0.62843, time= 25.14676
[2022/4/17 19:31:04] Epoch: 18, train_loss= 2.42863, train_acc= 0.68767, val_loss= 2.55069, val_acc= 0.63005, time= 17.48582
[2022/4/17 19:31:27] Epoch: 19, train_loss= 2.42451, train_acc= 0.68964, val_loss= 2.55028, val_acc= 0.64136, time= 22.59454
[2022/4/17 19:31:53] Epoch: 20, train_loss= 2.42051, train_acc= 0.69539, val_loss= 2.54991, val_acc= 0.64943, time= 26.73328
[2022/4/17 19:32:20] Epoch: 21, train_loss= 2.41695, train_acc= 0.70813, val_loss= 2.54959, val_acc= 0.65913, time= 26.88555
[2022/4/17 19:32:43] Epoch: 22, train_loss= 2.41378, train_acc= 0.71531, val_loss= 2.54929, val_acc= 0.66721, time= 22.62349
[2022/4/17 19:33:05] Epoch: 23, train_loss= 2.41053, train_acc= 0.72339, val_loss= 2.54899, val_acc= 0.66721, time= 22.42800
[2022/4/17 19:33:26] Epoch: 24, train_loss= 2.40715, train_acc= 0.73147, val_loss= 2.54873, val_acc= 0.66721, time= 20.79441
[2022/4/17 19:33:47] Epoch: 25, train_loss= 2.40399, train_acc= 0.73398, val_loss= 2.54850, val_acc= 0.67528, time= 21.29220
[2022/4/17 19:34:09] Epoch: 26, train_loss= 2.40115, train_acc= 0.73811, val_loss= 2.54829, val_acc= 0.67690, time= 21.21924
[2022/4/17 19:34:30] Epoch: 27, train_loss= 2.39853, train_acc= 0.74637, val_loss= 2.54807, val_acc= 0.68013, time= 21.55256
[2022/4/17 19:34:52] Epoch: 28, train_loss= 2.39594, train_acc= 0.75103, val_loss= 2.54784, val_acc= 0.67851, time= 21.77380
[2022/4/17 19:35:18] Epoch: 29, train_loss= 2.39326, train_acc= 0.75480, val_loss= 2.54761, val_acc= 0.68174, time= 25.74769
[2022/4/17 19:35:39] Epoch: 30, train_loss= 2.39066, train_acc= 0.75516, val_loss= 2.54741, val_acc= 0.69144, time= 21.70128
[2022/4/17 19:35:53] Epoch: 31, train_loss= 2.38834, train_acc= 0.75911, val_loss= 2.54723, val_acc= 0.69305, time= 13.85626
[2022/4/17 19:36:18] Epoch: 32, train_loss= 2.38611, train_acc= 0.76414, val_loss= 2.54704, val_acc= 0.70436, time= 24.59309
[2022/4/17 19:36:45] Epoch: 33, train_loss= 2.38384, train_acc= 0.77311, val_loss= 2.54689, val_acc= 0.70436, time= 27.12782
[2022/4/17 19:37:08] Epoch: 34, train_loss= 2.38161, train_acc= 0.77760, val_loss= 2.54676, val_acc= 0.70759, time= 22.58099
[2022/4/17 19:37:26] Epoch: 35, train_loss= 2.37949, train_acc= 0.78388, val_loss= 2.54664, val_acc= 0.70759, time= 18.67866
[2022/4/17 19:37:45] Epoch: 36, train_loss= 2.37752, train_acc= 0.78693, val_loss= 2.54652, val_acc= 0.71244, time= 18.50350
[2022/4/17 19:38:07] Epoch: 37, train_loss= 2.37559, train_acc= 0.78962, val_loss= 2.54636, val_acc= 0.71729, time= 22.24906
[2022/4/17 19:38:23] Epoch: 38, train_loss= 2.37362, train_acc= 0.79465, val_loss= 2.54620, val_acc= 0.72375, time= 15.93402
[2022/4/17 19:38:39] Epoch: 39, train_loss= 2.37172, train_acc= 0.79591, val_loss= 2.54606, val_acc= 0.73506, time= 15.82848
[2022/4/17 19:38:51] Epoch: 40, train_loss= 2.36993, train_acc= 0.79842, val_loss= 2.54594, val_acc= 0.73344, time= 11.73348
[2022/4/17 19:39:02] Epoch: 41, train_loss= 2.36820, train_acc= 0.80434, val_loss= 2.54583, val_acc= 0.73344, time= 11.68034
[2022/4/17 19:39:19] Epoch: 42, train_loss= 2.36652, train_acc= 0.80775, val_loss= 2.54573, val_acc= 0.73667, time= 17.16416
[2022/4/17 19:39:44] Epoch: 43, train_loss= 2.36485, train_acc= 0.81152, val_loss= 2.54563, val_acc= 0.73667, time= 24.27447
[2022/4/17 19:40:01] Epoch: 44, train_loss= 2.36323, train_acc= 0.81601, val_loss= 2.54554, val_acc= 0.74152, time= 17.13153
[2022/4/17 19:40:19] Epoch: 45, train_loss= 2.36165, train_acc= 0.81817, val_loss= 2.54545, val_acc= 0.74798, time= 18.42400
[2022/4/17 19:40:35] Epoch: 46, train_loss= 2.36010, train_acc= 0.82301, val_loss= 2.54534, val_acc= 0.74313, time= 16.07551
[2022/4/17 19:40:58] Epoch: 47, train_loss= 2.35859, train_acc= 0.82660, val_loss= 2.54523, val_acc= 0.75121, time= 22.33359
[2022/4/17 19:41:19] Epoch: 48, train_loss= 2.35714, train_acc= 0.83181, val_loss= 2.54513, val_acc= 0.75283, time= 21.21734
[2022/4/17 19:41:36] Epoch: 49, train_loss= 2.35576, train_acc= 0.83647, val_loss= 2.54505, val_acc= 0.75767, time= 17.51904
[2022/4/17 19:41:53] Epoch: 50, train_loss= 2.35440, train_acc= 0.83989, val_loss= 2.54498, val_acc= 0.75283, time= 17.00716
[2022/4/17 19:42:10] Epoch: 51, train_loss= 2.35305, train_acc= 0.84348, val_loss= 2.54492, val_acc= 0.75283, time= 16.58945
[2022/4/17 19:42:29] Epoch: 52, train_loss= 2.35175, train_acc= 0.84724, val_loss= 2.54487, val_acc= 0.75444, time= 19.24663
[2022/4/17 19:42:52] Epoch: 53, train_loss= 2.35049, train_acc= 0.85066, val_loss= 2.54481, val_acc= 0.75606, time= 22.29178
[2022/4/17 19:43:11] Epoch: 54, train_loss= 2.34927, train_acc= 0.85371, val_loss= 2.54475, val_acc= 0.76252, time= 19.25754
[2022/4/17 19:43:26] Epoch: 55, train_loss= 2.34806, train_acc= 0.85748, val_loss= 2.54468, val_acc= 0.76252, time= 15.11709
[2022/4/17 19:43:46] Epoch: 56, train_loss= 2.34689, train_acc= 0.86160, val_loss= 2.54462, val_acc= 0.76252, time= 19.54459
[2022/4/17 19:44:02] Epoch: 57, train_loss= 2.34575, train_acc= 0.86771, val_loss= 2.54456, val_acc= 0.76737, time= 16.72462
[2022/4/17 19:44:15] Epoch: 58, train_loss= 2.34463, train_acc= 0.86932, val_loss= 2.54451, val_acc= 0.76898, time= 12.40392
[2022/4/17 19:44:25] Epoch: 59, train_loss= 2.34355, train_acc= 0.87130, val_loss= 2.54447, val_acc= 0.77060, time= 10.48884
[2022/4/17 19:44:35] Epoch: 60, train_loss= 2.34250, train_acc= 0.87453, val_loss= 2.54444, val_acc= 0.77060, time= 10.14102
[2022/4/17 19:44:48] Epoch: 61, train_loss= 2.34147, train_acc= 0.87794, val_loss= 2.54440, val_acc= 0.77060, time= 12.23187
[2022/4/17 19:44:59] Epoch: 62, train_loss= 2.34046, train_acc= 0.88153, val_loss= 2.54436, val_acc= 0.77060, time= 11.57434
[2022/4/17 19:45:12] Epoch: 63, train_loss= 2.33948, train_acc= 0.88458, val_loss= 2.54432, val_acc= 0.77544, time= 12.90261
[2022/4/17 19:45:23] Epoch: 64, train_loss= 2.33852, train_acc= 0.88691, val_loss= 2.54428, val_acc= 0.77706, time= 10.67305
[2022/4/17 19:45:35] Epoch: 65, train_loss= 2.33758, train_acc= 0.88907, val_loss= 2.54424, val_acc= 0.77706, time= 11.92152
[2022/4/17 19:45:45] Epoch: 66, train_loss= 2.33667, train_acc= 0.89050, val_loss= 2.54421, val_acc= 0.77383, time= 10.35889
[2022/4/17 19:45:56] Epoch: 67, train_loss= 2.33578, train_acc= 0.89248, val_loss= 2.54418, val_acc= 0.77221, time= 11.25215
[2022/4/17 19:46:08] Epoch: 68, train_loss= 2.33490, train_acc= 0.89463, val_loss= 2.54415, val_acc= 0.77383, time= 11.30871
[2022/4/17 19:46:19] Epoch: 69, train_loss= 2.33405, train_acc= 0.89679, val_loss= 2.54413, val_acc= 0.77060, time= 11.47901
[2022/4/17 19:46:33] Epoch: 70, train_loss= 2.33322, train_acc= 0.89679, val_loss= 2.54411, val_acc= 0.77060, time= 13.56839
[2022/4/17 19:46:43] Epoch: 71, train_loss= 2.33241, train_acc= 0.89930, val_loss= 2.54408, val_acc= 0.77060, time= 10.78716
[2022/4/17 19:46:54] Epoch: 72, train_loss= 2.33161, train_acc= 0.90163, val_loss= 2.54406, val_acc= 0.77060, time= 10.18020
[2022/4/17 19:47:07] Epoch: 73, train_loss= 2.33083, train_acc= 0.90397, val_loss= 2.54404, val_acc= 0.77383, time= 12.85363
[2022/4/17 19:47:19] Epoch: 74, train_loss= 2.33006, train_acc= 0.90594, val_loss= 2.54403, val_acc= 0.77383, time= 12.16943
[2022/4/17 19:47:30] Epoch: 75, train_loss= 2.32932, train_acc= 0.90899, val_loss= 2.54403, val_acc= 0.77383, time= 11.41002
[2022/4/17 19:47:41] Epoch: 76, train_loss= 2.32859, train_acc= 0.91061, val_loss= 2.54402, val_acc= 0.77383, time= 10.45184
[2022/4/17 19:47:50] Epoch: 77, train_loss= 2.32787, train_acc= 0.91187, val_loss= 2.54400, val_acc= 0.77383, time= 9.63763
[2022/4/17 19:48:06] Epoch: 78, train_loss= 2.32717, train_acc= 0.91456, val_loss= 2.54398, val_acc= 0.77221, time= 16.00138
[2022/4/17 19:48:21] Epoch: 79, train_loss= 2.32648, train_acc= 0.91617, val_loss= 2.54396, val_acc= 0.77544, time= 14.63614
[2022/4/17 19:48:33] Epoch: 80, train_loss= 2.32581, train_acc= 0.91815, val_loss= 2.54395, val_acc= 0.78029, time= 11.75602
[2022/4/17 19:48:44] Epoch: 81, train_loss= 2.32515, train_acc= 0.92048, val_loss= 2.54394, val_acc= 0.78029, time= 11.00017
[2022/4/17 19:48:57] Epoch: 82, train_loss= 2.32451, train_acc= 0.92192, val_loss= 2.54393, val_acc= 0.77868, time= 12.98064
[2022/4/17 19:49:10] Epoch: 83, train_loss= 2.32387, train_acc= 0.92389, val_loss= 2.54393, val_acc= 0.77868, time= 13.33568
[2022/4/17 19:49:21] Epoch: 84, train_loss= 2.32325, train_acc= 0.92515, val_loss= 2.54392, val_acc= 0.77868, time= 11.12601
[2022/4/17 19:49:31] Epoch: 85, train_loss= 2.32265, train_acc= 0.92730, val_loss= 2.54392, val_acc= 0.78029, time= 9.74877
[2022/4/17 19:49:41] Epoch: 86, train_loss= 2.32205, train_acc= 0.92964, val_loss= 2.54392, val_acc= 0.78352, time= 9.98856
[2022/4/17 19:49:53] Epoch: 87, train_loss= 2.32147, train_acc= 0.93035, val_loss= 2.54391, val_acc= 0.78029, time= 11.79316
[2022/4/17 19:50:05] Epoch: 88, train_loss= 2.32090, train_acc= 0.93125, val_loss= 2.54391, val_acc= 0.77868, time= 11.86827
[2022/4/17 19:50:15] Epoch: 89, train_loss= 2.32034, train_acc= 0.93376, val_loss= 2.54391, val_acc= 0.77868, time= 10.36340
[2022/4/17 19:50:25] Epoch: 90, train_loss= 2.31979, train_acc= 0.93592, val_loss= 2.54390, val_acc= 0.77868, time= 9.69175
[2022/4/17 19:50:36] Epoch: 91, train_loss= 2.31925, train_acc= 0.93717, val_loss= 2.54390, val_acc= 0.77868, time= 11.00610
[2022/4/17 19:50:47] Epoch: 92, train_loss= 2.31872, train_acc= 0.93825, val_loss= 2.54390, val_acc= 0.77706, time= 11.28005
[2022/4/17 19:50:57] Epoch: 93, train_loss= 2.31820, train_acc= 0.94041, val_loss= 2.54390, val_acc= 0.77544, time= 10.36935
[2022/4/17 19:51:12] Epoch: 94, train_loss= 2.31769, train_acc= 0.94166, val_loss= 2.54390, val_acc= 0.77544, time= 14.58767
[2022/4/17 19:51:27] Epoch: 95, train_loss= 2.31719, train_acc= 0.94274, val_loss= 2.54390, val_acc= 0.77544, time= 15.28029
[2022/4/17 19:51:46] Epoch: 96, train_loss= 2.31670, train_acc= 0.94418, val_loss= 2.54390, val_acc= 0.77544, time= 18.63445
[2022/4/17 19:52:11] Epoch: 97, train_loss= 2.31622, train_acc= 0.94669, val_loss= 2.54390, val_acc= 0.77383, time= 25.34793
[2022/4/17 19:52:11] Early stopping...
[2022/4/17 19:52:11] Optimization Finished!
[2022/4/17 19:52:20] 	 loss= 2.50875, accuracy= 0.79683, time= 9.13013
[2022/4/17 19:52:20] Test Precision, Recall and F1-Score...
[2022/4/17 19:52:20] 
[2022/4/17 19:52:20] 
[2022/4/17 19:52:20] 
[2022/4/17 19:52:20] Macro average Test Precision, Recall and F1-Score...
[2022/4/17 19:52:20] (0.8085054464841267, 0.734217846685878, 0.7637367728974311, None)
[2022/4/17 19:52:20] Micro average Test Precision, Recall and F1-Score...
[2022/4/17 19:52:20] (0.7968273337400854, 0.7968273337400854, 0.7968273337400854, None)
[2022/4/17 19:52:20] Embeddings:
[2022/4/17 19:52:20] 
Word_embeddings:44053
[2022/4/17 19:52:20] 
Train_doc_embeddings:6190
[2022/4/17 19:52:20] 
Test_doc_embeddings:1639
[2022/4/17 19:52:20] 
Word_embeddings:


[2022/4/17 19:52:11] Early stopping...
 48%|████████████████████▏                     | 96/200 [27:03<29:19, 16.91s/it]
[2022/4/17 19:52:11] Optimization Finished!
[2022/4/17 19:52:20] Test set results: 
[2022/4/17 19:52:20] 	 loss= 2.50875, accuracy= 0.79683, time= 9.13013
[2022/4/17 19:52:20] Test Precision, Recall and F1-Score...
[2022/4/17 19:52:20]               precision    recall  f1-score   support
[2022/4/17 19:52:20] 
[2022/4/17 19:52:20]            0     0.8153    0.8603    0.8372       272
[2022/4/17 19:52:20]            1     0.8000    0.6154    0.6957        13
[2022/4/17 19:52:20]            2     0.8329    0.9014    0.8658       365
[2022/4/17 19:52:20]            3     0.7041    0.6216    0.6603       222
[2022/4/17 19:52:20]            4     0.8533    0.9143    0.8828        70
[2022/4/17 19:52:20]            5     0.9167    0.6471    0.7586        17
[2022/4/17 19:52:20]            7     0.6190    0.4815    0.5417        54
[2022/4/17 19:52:20]            8     0.7720    0.8338    0.8017       337
[2022/4/17 19:52:20]            9     0.8898    0.8537    0.8714       123
[2022/4/17 19:52:20]           10     0.7681    0.8281    0.7970        64
[2022/4/17 19:52:20]           11     0.7308    0.4935    0.5891        77
[2022/4/17 19:52:20]           12     1.0000    0.7600    0.8636        25
[2022/4/17 19:52:20] 
[2022/4/17 19:52:20]     accuracy                         0.7968      1639
[2022/4/17 19:52:20]    macro avg     0.8085    0.7342    0.7637      1639
[2022/4/17 19:52:20] weighted avg     0.7939    0.7968    0.7923      1639
[2022/4/17 19:52:20] 
[2022/4/17 19:52:20] Macro average Test Precision, Recall and F1-Score...
[2022/4/17 19:52:20] (0.8085054464841267, 0.734217846685878, 0.7637367728974311, None)
[2022/4/17 19:52:20] Micro average Test Precision, Recall and F1-Score...
[2022/4/17 19:52:20] (0.7968273337400854, 0.7968273337400854, 0.7968273337400854, None)
[2022/4/17 19:52:20] Embeddings:
Word_embeddings:44053
Train_doc_embeddings:6190
Test_doc_embeddings:1639
Word_embeddings::20] 
[[0.12418464 0.08030592 0.         ... 0.         0.         0.02454729]
 [0.         0.02609194 0.         ... 0.08601489 0.23542961 0.13335656]
 [0.35018283 0.26016787 0.54162043 ... 0.09087046 0.4690142  0.25934175]
 ...
 [0.00262535 0.         0.         ... 0.14062935 0.         0.03305604]
 [0.3524502  0.         0.03741288 ... 0.11292348 0.10991143 0.12000769]
 [0.10353006 0.24205425 0.25481513 ... 0.0956269  0.05245896 0.12194265]]
     1679.40 real      6764.05 user       867.80 sys
         11138252800  maximum resident set size
                   0  average shared memory size
                   0  average unshared data size
                   0  average unshared stack size
           176804492  page reclaims
                3246  page faults
                   0  swaps
                   0  block input operations
                   0  block output operations
                   0  messages sent
                   0  messages received
                   0  signals received
               82086  voluntary context switches
            29827551  involuntary context switches
      44549154881681  instructions retired
      24755464951944  cycles elapsed
         33933307904  peak memory footprint